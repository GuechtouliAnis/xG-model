{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import GBTClassifier, NaiveBayes, DecisionTreeClassifier, LinearSVC\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/27 14:00:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/27 14:00:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = \"hdfs://localhost:9000/user/aniss/events.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "events = spark.read.csv(hdfs_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['other_pp','from_fk','from_ti','from_corner','from_counter','from_gk','from_keeper','from_ko',\n",
    "            'header','corner_type','fk_type','pk_type',\n",
    "            'half_volley_technique','volley_technique','lob_technique','overhead_technique','backheel_technique',\n",
    "            'diving_h_technique',\n",
    "            'distance_to_goal', 'shot_angle', 'preferred_foot_shot', 'under_pressure',\n",
    "            'shot_aerial_won','shot_first_time','shot_one_on_one','shot_open_goal','shot_follows_dribble','players_inside_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xG_preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = pp.pre_training(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"goal\", featuresCol=\"features_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .build())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"goal\", metricName=\"areaUnderROC\")\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/27 14:12:12 WARN CacheManager: Asked to cache already cached data.\n",
      "25/01/27 14:12:12 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.8045916546331364\n"
     ]
    }
   ],
   "source": [
    "cvModel = crossval.fit(train_data)\n",
    "predictions = cvModel.transform(test_data)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test set accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = pp.pre_training(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"goal\", metricName=\"accuracy\")\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=accuracy_evaluator,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9011929247223365\n"
     ]
    }
   ],
   "source": [
    "cvModel = crossval.fit(train_data)\n",
    "predictions = cvModel.transform(test_data)\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Test set accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(self, models=['logistic'], param_grids=None, evaluator=None, num_folds=3, process=False, time=False):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning using CrossValidator for one or more models and return the results as a DataFrame.\n",
    "        \n",
    "        :param models: List of model names (str) to tune. Defaults to ['logistic'].\n",
    "        :param param_grids: Dictionary containing parameter grids for each model. If None, defaults to pre-defined grids.\n",
    "        :param evaluator: Evaluator to use for the validation. Defaults to BinaryClassificationEvaluator.\n",
    "        :param num_folds: Number of folds for cross-validation. Default is 3.\n",
    "        :param process: Boolean flag to control whether to print the process during tuning. Defaults to False.\n",
    "        :param time: Boolean flag to control whether to print the time taken for tuning. Defaults to False.\n",
    "        :return: DataFrame with columns ['model', 'params_dict', 'score'] sorted by score (highest to lowest).\n",
    "        \"\"\"\n",
    "            \n",
    "        if param_grids is None:\n",
    "            # Define default hyperparameter grids for each model\n",
    "            param_grids = {\n",
    "                'logistic': ParamGridBuilder().addGrid(LogisticRegression.regParam, [0.0, 0.1, 0.2]).addGrid(LogisticRegression.maxIter, [10, 50, 100]).build(),\n",
    "                'rf': ParamGridBuilder().addGrid(RandomForestClassifier.numTrees, [50, 100, 200]).addGrid(RandomForestClassifier.maxDepth, [5, 10, 15]).build(),\n",
    "                'mlp': ParamGridBuilder().addGrid(MultilayerPerceptronClassifier.maxIter, [50, 100]).addGrid(MultilayerPerceptronClassifier.layers, [[len(self.FEATURES), 10, 2], [len(self.FEATURES), 20, 2]]).build(),\n",
    "                'gbt': ParamGridBuilder().addGrid(GBTClassifier.maxIter, [50, 100]).addGrid(GBTClassifier.maxDepth, [5, 10]).build(),\n",
    "                'nb': ParamGridBuilder().addGrid(NaiveBayes.smoothing, [1.0, 1.5, 2.0]).build(),\n",
    "                'dt': ParamGridBuilder().addGrid(DecisionTreeClassifier.maxDepth, [5, 10, 15]).addGrid(DecisionTreeClassifier.minInstancesPerNode, [1, 2, 3]).build(),\n",
    "                'svm': ParamGridBuilder().addGrid(LinearSVC.regParam, [0.0, 0.1, 0.2]).addGrid(LinearSVC.maxIter, [50, 100]).build()\n",
    "            }\n",
    "\n",
    "        if evaluator is None:\n",
    "            evaluator = BinaryClassificationEvaluator(labelCol=self.label_col, rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for model_name in models:\n",
    "            model = self.initialize_model()  # Initialize the model as per the selected model\n",
    "            param_grid = param_grids[model_name]\n",
    "\n",
    "            if process:\n",
    "                print(f\"Tuning hyperparameters for {model_name}...\")\n",
    "\n",
    "            # Perform cross-validation\n",
    "            cross_val = CrossValidator(estimator=model, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=num_folds)\n",
    "            cv_model = cross_val.fit(self.train_data)\n",
    "\n",
    "            # Collect results for each parameter set\n",
    "            for i, params in enumerate(cv_model.getEstimatorParamMaps()):\n",
    "                model_score = evaluator.evaluate(cv_model.transform(self.test_data))\n",
    "\n",
    "                # Convert the ParamGrid to a clean dictionary for better readability\n",
    "                params_dict = {param.name: value for param, value in params.items()}\n",
    "\n",
    "                # Store the result for this parameter set\n",
    "                results.append({\n",
    "                    'model': model_name,\n",
    "                    'params_dict': params_dict,\n",
    "                    'score': model_score\n",
    "                })\n",
    "\n",
    "                if process:\n",
    "                    print(f\"Iteration {i+1}: Score for params {params_dict} is {model_score:.4f}\")\n",
    "\n",
    "        # Convert the results to a DataFrame and sort by score in descending order\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df = results_df.sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        if process:\n",
    "            print(\"Hyperparameter tuning completed.\")\n",
    "\n",
    "        return results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
