{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['other_pp','from_fk','from_ti','from_corner','from_counter','from_gk','from_keeper','from_ko',\n",
    "            'header','corner_type','fk_type','pk_type',\n",
    "            'half_volley_technique','volley_technique','lob_technique','overhead_technique','backheel_technique','diving_h_technique',\n",
    "            'distance_to_goal', 'shot_angle', 'preferred_foot_shot', 'under_pressure',\n",
    "            'shot_aerial_won','shot_first_time','shot_one_on_one','shot_open_goal','shot_follows_dribble','players_inside_area']\n",
    "target = ['goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/26 02:17:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"xG6\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = spark.read.csv('Datas/events_shot.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pre_training as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = pt.pre_training(events, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = pt.train_model(train_data,'gbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trained_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, format_number\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define the function to extract the second element from the probability list\n",
    "def extract_goal_probability(probability):\n",
    "    return float(probability[1])\n",
    "\n",
    "# Register the function as a UDF\n",
    "extract_goal_probability_udf = udf(extract_goal_probability, DoubleType())\n",
    "\n",
    "# Overwrite the prediction column using the UDF\n",
    "predictions = predictions.withColumn(\"goal_probability\", extract_goal_probability_udf(col(\"probability\")))\n",
    "\n",
    "# Format the goal_probability to remove scientific notation\n",
    "predictions = predictions.withColumn(\"goal_probability\", format_number(col(\"goal_probability\"), 10))\n",
    "# convert goal_probability to float\n",
    "predictions = predictions.withColumn(\"goal_probability\", col(\"goal_probability\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation import ModelEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = ModelEvaluation(predictions,'goal','prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "me_r = ModelEvaluation(predictions,'shot_statsbomb_xg','goal_probability',model_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 0.0040395032176577285,\n",
       " 'RMSE': 0.06355708628986802,\n",
       " 'MAE': 0.04298674341053067,\n",
       " 'R2': 0.8129201469995636}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_r.get_all_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9018510900863842,\n",
       " 'Precision': 0.7774086378737541,\n",
       " 'Recall': 0.17205882352941176,\n",
       " 'Sensitivity': 0.17205882352941176,\n",
       " 'Specificity': 0.9937934228809634,\n",
       " 'F1': 0.28175797712221556,\n",
       " 'FPR': 0.006206577119036591,\n",
       " 'FNR': 0.8279411764705882}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me.get_all_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
